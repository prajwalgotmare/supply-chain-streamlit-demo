{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data ---\n",
      "Successfully loaded data/spoilage_data.csv with 500 rows.\n",
      "\n",
      "--- Step 2: Performing Feature Engineering ---\n",
      "New features created: ['temp_x_hours', 'temp_squared']\n",
      "\n",
      "--- Step 3: Preprocessing Data ---\n",
      "Categorical feature 'sku_id' has been one-hot encoded.\n",
      "\n",
      "--- Step 4: Training Final Model on FULL Dataset ---\n",
      "Final model training complete.\n",
      "\n",
      "--- Step 5: Saving Model and Column List for Deployment ---\n",
      "  -> Model saved to 'spoilage_model.pkl'\n",
      "  -> Model columns saved to 'spoilage_columns.json'\n",
      "\n",
      "✅ Pipeline execution finished successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle  # Use Python's native pickle library\n",
    "import json    # For saving the column list\n",
    "\n",
    "# --- Configuration ---\n",
    "# Let's use clean, descriptive filenames.\n",
    "DATA_FILE = 'data/spoilage_data.csv' \n",
    "MODEL_FILENAME = 'spoilage_model.pkl'      # Use .pkl extension for pickle files\n",
    "COLUMNS_FILENAME = 'spoilage_columns.json'\n",
    "\n",
    "def run_training_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the full pipeline for creating the final deployment model:\n",
    "    1. Load Data\n",
    "    2. Engineer Features\n",
    "    3. Preprocess Data (One-Hot Encode)\n",
    "    4. Train Model on FULL Data\n",
    "    5. Save Model and Columns using stable methods\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Load Data ---\n",
    "    print(\"--- Step 1: Loading Data ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        print(f\"Successfully loaded {DATA_FILE} with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {DATA_FILE} not found. Please ensure it's in the same directory as the script.\")\n",
    "        return\n",
    "\n",
    "    # --- Step 2: Feature Engineering ---\n",
    "    print(\"\\n--- Step 2: Performing Feature Engineering ---\")\n",
    "    df['temp_x_hours'] = df['avg_temp'] * df['transit_hours']\n",
    "    df['temp_squared'] = df['avg_temp']**2\n",
    "    print(\"New features created: ['temp_x_hours', 'temp_squared']\")\n",
    "\n",
    "    # --- Step 3: Data Preprocessing ---\n",
    "    print(\"\\n--- Step 3: Preprocessing Data ---\")\n",
    "    X = df.drop(columns=['shipment_id', 'spoilage_flag']) \n",
    "    y = df['spoilage_flag']\n",
    "    X_encoded = pd.get_dummies(X, columns=['sku_id'], prefix='sku')\n",
    "    print(\"Categorical feature 'sku_id' has been one-hot encoded.\")\n",
    "\n",
    "    # --- Step 4: Train Model on FULL Data ---\n",
    "    # We train on 100% of the data to make the final model as robust as possible.\n",
    "    # The train/test split was only for getting the performance metrics.\n",
    "    print(\"\\n--- Step 4: Training Final Model on FULL Dataset ---\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    model.fit(X_encoded, y)\n",
    "    print(\"Final model training complete.\")\n",
    "\n",
    "    # --- Step 5: Save Model and Supporting Files ---\n",
    "    print(\"\\n--- Step 5: Saving Model and Column List for Deployment ---\")\n",
    "    \n",
    "    # Save the trained model object using pickle with a compatible protocol\n",
    "    with open(MODEL_FILENAME, 'wb') as f:\n",
    "        pickle.dump(model, f, protocol=4)\n",
    "    print(f\"  -> Model saved to '{MODEL_FILENAME}'\")\n",
    "\n",
    "    # Save the list of column names as a JSON file\n",
    "    model_columns = X_encoded.columns.tolist()\n",
    "    with open(COLUMNS_FILENAME, 'w') as f:\n",
    "        json.dump(model_columns, f)\n",
    "    print(f\"  -> Model columns saved to '{COLUMNS_FILENAME}'\")\n",
    "\n",
    "    print(\"\\n✅ Pipeline execution finished successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
