{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Loading Data ---\n",
      "Successfully loaded data/spoilage_data.csv with 500 rows.\n",
      "\n",
      "--- Step 2: Performing Feature Engineering ---\n",
      "New features created: ['temp_x_hours', 'temp_squared']\n",
      "\n",
      "--- Step 3: Preprocessing Data ---\n",
      "Categorical feature 'sku_id' has been one-hot encoded.\n",
      "\n",
      "--- Step 5: Training Final Model on FULL Dataset ---\n",
      "Final model training complete.\n",
      "\n",
      "--- Step 7: Saving Model and Column List for Deployment ---\n",
      "  -> Model saved to 'spoilage_model.joblib'\n",
      "  -> Model columns saved to 'model_columns.json'\n",
      "\n",
      "✅ Pipeline execution finished successfully!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_FILE = 'data/spoilage_data.csv'\n",
    "MODEL_FILENAME = 'spoilage_model.joblib'\n",
    "COLUMNS_FILENAME = 'model_columns.json'\n",
    "\n",
    "def run_training_pipeline():\n",
    "    \"\"\"\n",
    "    Executes the full pipeline:\n",
    "    1. Load Data\n",
    "    2. Engineer Features\n",
    "    3. Preprocess Data (One-Hot Encode)\n",
    "    4. Split Data\n",
    "    5. Train Model\n",
    "    6. Evaluate Model\n",
    "    7. Save Model and Columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Step 1: Load Data & Initial Exploration ---\n",
    "    print(\"--- Step 1: Loading Data ---\")\n",
    "    try:\n",
    "        df = pd.read_csv(DATA_FILE)\n",
    "        print(f\"Successfully loaded {DATA_FILE} with {len(df)} rows.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {DATA_FILE} not found. Please make sure it's in the same directory.\")\n",
    "        return\n",
    "\n",
    "    # Basic Exploration\n",
    "    # print(df.head())\n",
    "    # print(df.info())\n",
    "    # print(df['spoilage_flag'].value_counts(normalize=True))\n",
    "    \n",
    "    # --- Step 2: Feature Engineering ---\n",
    "    print(\"\\n--- Step 2: Performing Feature Engineering ---\")\n",
    "    # Interaction feature between temperature and transit time\n",
    "    df['temp_x_hours'] = df['avg_temp'] * df['transit_hours']\n",
    "    # Non-linear temperature feature to penalize high temperatures more\n",
    "    df['temp_squared'] = df['avg_temp']**2\n",
    "    print(\"New features created: ['temp_x_hours', 'temp_squared']\")\n",
    "\n",
    "    # --- Step 3: Data Preprocessing ---\n",
    "    print(\"\\n--- Step 3: Preprocessing Data ---\")\n",
    "    # Define features (X) and target (y)\n",
    "    X = df.drop(columns=['shipment_id', 'spoilage_flag']) \n",
    "    y = df['spoilage_flag']\n",
    "\n",
    "    # One-Hot Encode the 'sku_id' feature\n",
    "    X_encoded = pd.get_dummies(X, columns=['sku_id'], prefix='sku')\n",
    "    print(\"Categorical feature 'sku_id' has been one-hot encoded.\")\n",
    "    # print(X_encoded.head())\n",
    "\n",
    "    # # --- Step 4: Split Data ---\n",
    "    # print(\"\\n--- Step 4: Splitting Data into Training and Testing Sets ---\")\n",
    "    # X_train, X_test, y_train, y_test = train_test_split(\n",
    "    #     X_encoded, y, \n",
    "    #     test_size=0.2,       # 20% of data will be for testing\n",
    "    #     random_state=42,     # Ensures reproducibility\n",
    "    #     stratify=y           # Ensures train/test sets have similar spoilage ratios\n",
    "    # )\n",
    "    # print(f\"Data split into {len(X_train)} training rows and {len(X_test)} testing rows.\")\n",
    "\n",
    "    # --- Step 5: Model Training on FULL Data ---\n",
    "    print(\"\\n--- Step 5: Training Final Model on FULL Dataset ---\")\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=200,\n",
    "        max_depth=15,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Fit the model on ALL of X_encoded and y\n",
    "    model.fit(X_encoded, y)\n",
    "    print(\"Final model training complete.\")\n",
    "\n",
    "    # --- Step 6: Model Evaluation ---\n",
    "    # print(\"\\n--- Step 6: Evaluating Model Performance ---\")\n",
    "    # # Make predictions on the unseen test set\n",
    "    # y_pred = model.predict(X_test)\n",
    "    # y_pred_proba = model.predict_proba(X_test)[:, 1] # Probabilities for the \"spoiled\" class\n",
    "\n",
    "    # # Calculate performance metrics\n",
    "    # accuracy = accuracy_score(y_test, y_pred)\n",
    "    # roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "    # print(f\"  -> Accuracy on Test Set:      {accuracy:.4f}\")\n",
    "    # print(f\"  -> ROC-AUC Score on Test Set: {roc_auc:.4f}\")\n",
    "\n",
    "    # # Display feature importances to understand the model's decisions\n",
    "    # print(\"\\n--- Top 10 Feature Importances ---\")\n",
    "    # importances = pd.DataFrame({\n",
    "    #     'feature': X_encoded.columns,\n",
    "    #     'importance': model.feature_importances_\n",
    "    # }).sort_values('importance', ascending=False)\n",
    "    # print(importances.head(10))\n",
    "\n",
    "    # --- Step 7: Save Model and Supporting Files ---\n",
    "    print(\"\\n--- Step 7: Saving Model and Column List for Deployment ---\")\n",
    "    \n",
    "    # Save the trained model object\n",
    "    joblib.dump(model, MODEL_FILENAME)\n",
    "    print(f\"  -> Model saved to '{MODEL_FILENAME}'\")\n",
    "\n",
    "    # Save the list of column names in the exact order\n",
    "    model_columns = X_encoded.columns.tolist()\n",
    "    with open(COLUMNS_FILENAME, 'w') as f:\n",
    "        json.dump(model_columns, f)\n",
    "    print(f\"  -> Model columns saved to '{COLUMNS_FILENAME}'\")\n",
    "    \n",
    "    print(\"\\n✅ Pipeline execution finished successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_training_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
